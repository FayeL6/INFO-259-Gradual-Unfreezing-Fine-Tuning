{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "aa014ff2-9f74-4e05-b8a6-3dc0b864a3bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdabcbc",
   "metadata": {},
   "source": [
    "# NER task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "90fffe82-c6de-4c7a-8dc2-fbb2e550e878",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on cuda\n"
     ]
    }
   ],
   "source": [
    "# Check the available device and use GPU if available, otherwise use CPU\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "# Print the device being used\n",
    "print(f'Working on {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052820ab-f4df-4bcf-8104-677bbf973ffd",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b5d2f94e-f270-4f37-ab88-bcb9c955573e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 14041\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 3250\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 3453\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the CoNLL-2003 dataset using the 'datasets' library.\n",
    "dataset = load_dataset('conll2003')\n",
    "label_names = dataset['train'].features['ner_tags'].feature.names\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "086dc1d2-f002-41ba-96b6-5a5351d65f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Create a tokenizer instance by loading the pre-trained model.\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9f571a18-39d1-4db9-9362-9f07ca21df53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def align_target(labels, word_ids):\n",
    "    # Define a mapping from beginning (B-) labels to inside (I-) labels\n",
    "    begin2inside = {\n",
    "        1: 2,  # B-LOC -> I-LOC\n",
    "        3: 4,  # B-MISC -> I-MISC\n",
    "        5: 6,  # B-ORG -> I-ORG\n",
    "        7: 8    # B-PER -> I-PER\n",
    "    }\n",
    "\n",
    "    # Initialize an empty list to store aligned labels and a variable to track the last word\n",
    "    align_labels = []\n",
    "    last_word = None\n",
    "\n",
    "    # Iterate through the word_ids\n",
    "    for word in word_ids:\n",
    "        if word is None:\n",
    "            label = -100  # Set label to -100 for None word_ids\n",
    "        elif word != last_word:\n",
    "            label = labels[word]  # Use the label corresponding to the current word_id\n",
    "        else:\n",
    "            label = labels[word]\n",
    "            # Change B- to I- if the previous word is the same\n",
    "            if label in begin2inside:\n",
    "                label = begin2inside[label]  # Map B- to I-\n",
    "\n",
    "        # Append the label to the align_labels list and update last_word\n",
    "        align_labels.append(label)\n",
    "        last_word = word\n",
    "\n",
    "    return align_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "641c14d9-e0e2-4f75-96f3-405f5dd3252c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_fn(batch):\n",
    "    # Tokenize the input batch\n",
    "    tokenized_inputs = tokenizer(batch['tokens'], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    # Extract the labels batch from the input batch\n",
    "    labels_batch = batch['ner_tags']\n",
    "\n",
    "    # Initialize a list to store aligned targets for each example in the batch\n",
    "    aligned_targets_batch = []\n",
    "\n",
    "    # Iterate through each example and align the labels\n",
    "    for i, labels in enumerate(labels_batch):\n",
    "        # Extract the word_ids for the current example\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "\n",
    "        # Use the align_target function to align the labels\n",
    "        aligned_targets_batch.append(align_target(labels, word_ids))\n",
    "\n",
    "    # Add the aligned labels to the tokenized inputs under the key \"labels\"\n",
    "    tokenized_inputs[\"labels\"] = aligned_targets_batch\n",
    "\n",
    "    # Return the tokenized inputs, including aligned labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c73608ed-02ee-46fe-bcf5-812419f2a22c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cf4d1c0ef1f4bdbbafce5f47de11f9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(tokenize_fn, batched=True, remove_columns=dataset['train'].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9f14574c-2a8c-4197-9733-fb66e1cb6962",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "# Create a DataCollatorForTokenClassification object\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "# Testing data using the data collator\n",
    "batch = data_collator([tokenized_dataset['train'][i] for i in range(2)])\n",
    "\n",
    "# Display the resulting batch\n",
    "# batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36e202b-a15a-45c5-9721-6e011e62b03a",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1cccff84-32bc-492e-b497-eec6d632b32c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/accounts/grad/fangyuan_li/.local/lib/python3.11/site-packages/datasets/load.py:759: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Import the seqeval metric from Hugging Face's datasets library\n",
    "from datasets import load_metric  \n",
    "\n",
    "# Load the seqeval metric which can evaluate NER and other sequence tasks\n",
    "metric = load_metric(\"seqeval\")\n",
    "\n",
    "# Function to compute evaluation metrics from model logits and true labels\n",
    "def compute_metrics(logits_and_labels):\n",
    "    \n",
    "    # Unpack the logits and labels\n",
    "    logits, labels = logits_and_labels \n",
    "    \n",
    "    # Get predictions from the logits\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    # Remove ignored index (special tokens)\n",
    "    str_labels = [\n",
    "    [label_names[t] for t in label if t!=-100] for label in labels\n",
    "    ]\n",
    "\n",
    "    str_preds = [\n",
    "    [label_names[p] for (p, t) in zip(prediction, label) if t != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    \n",
    "    # Compute metrics\n",
    "    results = metric.compute(predictions=str_preds, references=str_labels) \n",
    "    \n",
    "    # Extract key metrics\n",
    "    return {\n",
    "    \"precision\": results[\"overall_precision\"],\n",
    "    \"recall\": results[\"overall_recall\"], \n",
    "    \"f1\": results[\"overall_f1\"],\n",
    "    \"accuracy\": results[\"overall_accuracy\"]  \n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3aea927-1ca2-45c1-9420-7d0c857454f5",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "536ea5c9-dac9-4950-82af-19d702bede67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mapping from label ID to label string name\n",
    "id2label = {k: v for k, v in enumerate(label_names)} \n",
    "\n",
    "# Create reverse mapping from label name to label ID\n",
    "label2id = {v: k for k, v in enumerate(label_names)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f15981a1-81c7-4e1d-99c3-b1966bbc2086",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load pretrained token classification model from Transformers \n",
    "from transformers import AutoModelForTokenClassification, BertForTokenClassification\n",
    "\n",
    "# Initialize model object with pretrained weights\n",
    "# model = AutoModelForTokenClassification.from_pretrained(\n",
    "#   checkpoint,\n",
    "\n",
    "#   # Pass in label mappings\n",
    "#   id2label=id2label,  \n",
    "#   label2id=label2id\n",
    "# )\n",
    "model = BertForTokenClassification.from_pretrained('bert-base-uncased', \n",
    "                                                   num_labels=len(id2label),\n",
    "                                                   id2label=id2label,\n",
    "                                                   label2id=label2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a5268b-5356-4393-b1b4-4b93b36c3c82",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Freeze Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9a8bb755-8901-485f-9369-2bcf1c2b4acd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e6d35140-a607-4f9d-a50e-3c58cf95e4d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def freeze_layers(model, num_layers_to_freeze):\n",
    "    \"\"\"\n",
    "    Freeze the first 'num_layers_to_freeze' layers of a model.\n",
    "\n",
    "    Args:\n",
    "    model (torch.nn.Module): The model whose layers are to be frozen.\n",
    "    num_layers_to_freeze (int): The number of layers to freeze.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Check for the typical attribute in BERT-like models\n",
    "    encoder_layers = model.bert.encoder.layer\n",
    "\n",
    "    # Freeze specified number of layers in the encoder\n",
    "    layer_count = 0\n",
    "    for layer in encoder_layers:\n",
    "        if layer_count < num_layers_to_freeze:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = False\n",
    "        layer_count += 1\n",
    "\n",
    "        # Break if we have frozen the desired number of layers\n",
    "        if layer_count >= num_layers_to_freeze:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "90deb77f-ce80-447f-8bdc-cd936ae8b896",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Freeze the first 11 layers of the model\n",
    "freeze_layers(model, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "728120c9-09a0-4555-b4c2-cdab8cc0d61d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1 is frozen.\n",
      "Layer 2 is frozen.\n",
      "Layer 3 is frozen.\n",
      "Layer 4 is frozen.\n",
      "Layer 5 is frozen.\n",
      "Layer 6 is frozen.\n",
      "Layer 7 is frozen.\n",
      "Layer 8 is frozen.\n",
      "Layer 9 is frozen.\n",
      "Layer 10 is frozen.\n",
      "Layer 11 is frozen.\n",
      "Layer 12 is not frozen.\n",
      "Some layers are not fully frozen.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def check_if_layers_are_frozen(model):\n",
    "    \"\"\"\n",
    "    Check if the encoder layers of a model are frozen.\n",
    "\n",
    "    Args:\n",
    "    model (torch.nn.Module): The model to check.\n",
    "\n",
    "    Returns:\n",
    "    bool: True if all encoder layers are frozen, False otherwise.\n",
    "    \"\"\"\n",
    "    encoder_layers = model.bert.encoder.layer\n",
    "    all_frozen = True\n",
    "    layer_count = 0\n",
    "\n",
    "    for layer in encoder_layers:\n",
    "        layer_frozen = True\n",
    "        for param in layer.parameters():\n",
    "            if param.requires_grad:\n",
    "                layer_frozen = False\n",
    "                all_frozen = False\n",
    "        print(f\"Layer {layer_count + 1} is {'frozen' if layer_frozen else 'not frozen'}.\")\n",
    "        layer_count += 1\n",
    "\n",
    "    if all_frozen:\n",
    "        print(\"All layers are frozen.\")\n",
    "    else:\n",
    "        print(\"Some layers are not fully frozen.\")\n",
    "\n",
    "    return all_frozen\n",
    "\n",
    "# Now, check if the layers are frozen\n",
    "check_if_layers_are_frozen(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532166dd-61f1-4d1c-970f-e6c888383a85",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Trainer Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "df15f773-b2e2-44e3-8099-de687795195c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/accounts/grad/fangyuan_li/.local/lib/python3.11/site-packages/transformers/training_args.py:1454: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Configure training arguments using TrainigArguments class\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "  # Location to save fine-tuned model \n",
    "  output_dir = \"fine_tuned_model\",\n",
    "\n",
    "  # Evaluate each epoch\n",
    "  evaluation_strategy = \"epoch\",\n",
    "\n",
    "  # Learning rate for Adam optimizer\n",
    "  learning_rate = 5e-5, \n",
    "  \n",
    "  # Batch sizes for training and evaluation\n",
    "  per_device_train_batch_size = 16,\n",
    "  per_device_eval_batch_size = 16,\n",
    "    \n",
    "  # Number of training epochs\n",
    "  num_train_epochs = 50,\n",
    "\n",
    "  # L2 weight decay regularization\n",
    "  weight_decay = 0.01,\n",
    "  \n",
    "  save_strategy=\"no\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994e1cec-c117-4b92-8488-f479845b8a4c",
   "metadata": {},
   "source": [
    "## Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "52434e63-2387-450b-b44d-6c17a517f0ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "\n",
    "class EarlyStoppingCallback(TrainerCallback):\n",
    "    def __init__(self, patience=3):\n",
    "        self.patience = patience\n",
    "        self.best_metric = float('-inf')\n",
    "        self.counter = 0\n",
    "\n",
    "    def on_evaluate(self, args, state, control, metrics, **kwargs):\n",
    "        # Extract the evaluation metric from the state\n",
    "        metric = metrics['eval_loss']  ## edit here\n",
    "        # Check if the metric has improved\n",
    "        # If improving, keep training\n",
    "        if metric < self.best_metric:\n",
    "            self.best_metric = metric\n",
    "            self.counter = 0\n",
    "            trainer.save_model('../NER/one_layer/') # edit here\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            # If no improvement after patience epochs, stop training\n",
    "            if self.counter >= self.patience:\n",
    "                control.should_training_stop = True\n",
    "                print(\"Training stopped due to lack of improvement.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "36f829d2-5c54-437b-a91e-7b805925ee1d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2634' max='43900' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 2634/43900 04:37 < 1:12:23, 9.50 it/s, Epoch 3/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.317100</td>\n",
       "      <td>0.093593</td>\n",
       "      <td>0.808401</td>\n",
       "      <td>0.871255</td>\n",
       "      <td>0.838652</td>\n",
       "      <td>0.973613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.082500</td>\n",
       "      <td>0.070523</td>\n",
       "      <td>0.856598</td>\n",
       "      <td>0.907775</td>\n",
       "      <td>0.881445</td>\n",
       "      <td>0.980047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.049800</td>\n",
       "      <td>0.065968</td>\n",
       "      <td>0.881252</td>\n",
       "      <td>0.919219</td>\n",
       "      <td>0.899835</td>\n",
       "      <td>0.982652</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training stopped due to lack of improvement.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2634, training_loss=0.12119111384780641, metrics={'train_runtime': 277.1959, 'train_samples_per_second': 2532.685, 'train_steps_per_second': 158.372, 'total_flos': 1020143109346326.0, 'train_loss': 0.12119111384780641, 'epoch': 3.0})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "model = BertForTokenClassification.from_pretrained('bert-base-uncased', \n",
    "                                                   num_labels=len(id2label),\n",
    "                                                   id2label=id2label,\n",
    "                                                   label2id=label2id)\n",
    "\n",
    "freeze_layers(model, 11) # edit here\n",
    "\n",
    "trainer = Trainer(\n",
    "  # Model to train\n",
    "  model=model, \n",
    "  \n",
    "  # Training arguments\n",
    "  args=training_args,\n",
    "\n",
    "  # Training and validation datasets\n",
    "  train_dataset=tokenized_dataset[\"train\"],\n",
    "  eval_dataset=tokenized_dataset[\"validation\"],\n",
    "\n",
    "  # Tokenizer\n",
    "  tokenizer=tokenizer,\n",
    "\n",
    "  # Custom metric function\n",
    "  compute_metrics=compute_metrics,\n",
    "\n",
    "  # Data collator\n",
    "  data_collator=data_collator,\n",
    "    \n",
    "  callbacks=[EarlyStoppingCallback(patience=3)]\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "#trainer.save_model('../ner/one_layer/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15227242-4633-4c0d-911e-8085471b5f2a",
   "metadata": {},
   "source": [
    "## Gradual Unfreezing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f2db14-79e0-4ac3-8fa7-75c4987059b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = BertForTokenClassification.from_pretrained('bert-base-uncased', \n",
    "                                                   num_labels=len(id2label),\n",
    "                                                   id2label=id2label,\n",
    "                                                   label2id=label2id)\n",
    "freeze_layers(model, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e5ace4-d536-460b-9e50-a74aca57d78c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "\n",
    "class GradualUnfreezingCallback(TrainerCallback):\n",
    "    def __init__(self, model, patience=2): # edit patience here\n",
    "        self.model = model\n",
    "        self.patience = patience\n",
    "        self.best_metric = float('inf')\n",
    "        self.counter = 0\n",
    "        self.num_freeze = 11\n",
    "                 \n",
    "    def unfreeze_last_layers(self, model, num_layers_to_unfreeze):\n",
    "                 \n",
    "        # Initially freeze all layers\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Check for the typical attribute in BERT-like models\n",
    "        encoder_layers = model.bert.encoder.layer\n",
    "        total_layers = len(encoder_layers)\n",
    "\n",
    "        # Unfreeze the specified number of last layers\n",
    "        layers_to_start_unfreezing = total_layers - num_layers_to_unfreeze\n",
    "\n",
    "        for i, layer in enumerate(encoder_layers):\n",
    "            if i >= layers_to_start_unfreezing:\n",
    "                for param in layer.parameters():\n",
    "                    param.requires_grad = True\n",
    "    \n",
    "    def check_if_layers_are_frozen(self, model):\n",
    "        encoder_layers = model.bert.encoder.layer\n",
    "        all_frozen = True\n",
    "        layer_count = 0\n",
    "\n",
    "        for layer in encoder_layers:\n",
    "            layer_frozen = True\n",
    "            for param in layer.parameters():\n",
    "                if param.requires_grad:\n",
    "                    layer_frozen = False\n",
    "                    all_frozen = False\n",
    "            print(f\"Layer {layer_count + 1} is {'frozen' if layer_frozen else 'not frozen'}.\")\n",
    "            layer_count += 1\n",
    "\n",
    "        if all_frozen:\n",
    "            print(\"All layers are frozen.\")\n",
    "        else:\n",
    "            print(\"Some layers are not fully frozen.\")\n",
    "\n",
    "        return all_frozen\n",
    "\n",
    "    def on_evaluate(self, args, state, control, metrics, **kwargs):\n",
    "        # Extract the evaluation metric from the state\n",
    "        metric = metrics['eval_loss']  ## edit here\n",
    "        # Check if the metric has improved\n",
    "        # If improving, keep training\n",
    "        if metric < self.best_metric:\n",
    "            self.best_metric = metric\n",
    "            self.counter = 0\n",
    "        # If not improving, unfreeze a layer or stop early\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            # If no improvement after patience epochs, stop training\n",
    "            if self.counter > self.patience:\n",
    "                control.should_training_stop = True\n",
    "                print(\"Training stopped due to lack of improvement.\")\n",
    "            else:\n",
    "                self.num_freeze -= 1\n",
    "                 \n",
    "                if self.num_freeze < 0:\n",
    "                    control.should_training_stop = True\n",
    "                    print('Stopping triggered. No improvement in validation loss on last layer')\n",
    "                else:\n",
    "                    self.unfreeze_last_layers(self.model,12-self.num_freeze)\n",
    "                    print(f'unfreezing layer {12-self.num_freeze}')\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "  # Model to train\n",
    "  model=model, \n",
    "  \n",
    "  # Training arguments\n",
    "  args=training_args,\n",
    "\n",
    "  # Training and validation datasets\n",
    "  train_dataset=tokenized_dataset[\"train\"],\n",
    "  eval_dataset=tokenized_dataset[\"validation\"],\n",
    "\n",
    "  # Tokenizer\n",
    "  tokenizer=tokenizer,\n",
    "\n",
    "  # Custom metric function\n",
    "  compute_metrics=compute_metrics,\n",
    "\n",
    "  # Data collator\n",
    "  data_collator=data_collator,\n",
    "    \n",
    "  callbacks=[GradualUnfreezingCallback(model=model, patience=2)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd74a5e4-f874-4f28-be4b-0acd9994f3cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "\n",
    "#trainer.save_model('../ner/one_layer/') # edit here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc8056e-7606-484a-a103-85bcc78e755f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
