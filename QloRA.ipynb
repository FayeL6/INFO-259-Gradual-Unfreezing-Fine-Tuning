{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e90b0dc5-5d47-4fc0-be88-bf5016fccf2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install auto-gptq\n",
    "# !pip install optimum\n",
    "# !pip install bitsandbytes\n",
    "# !pip install --upgrade git+https://github.com/huggingface/transformers\n",
    "# !pip install --upgrade torch torchvision\n",
    "# !pip install --upgrade accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2d0e28fe-34dd-4c08-a2a8-b1485dfebe1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from peft import prepare_model_for_kbit_training\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import transformers\n",
    "from transformers import BertTokenizerFast, BertForQuestionAnswering, TrainingArguments, Trainer\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    GenerationConfig\n",
    ")\n",
    "import torch\n",
    "import json\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cd43069b-bb27-4a8d-9fe1-b7a7cbafaf8e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on cuda\n"
     ]
    }
   ],
   "source": [
    "# Check the available device and use GPU if available, otherwise use CPU\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "# Print the device being used\n",
    "print(f'Working on {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5270a1de-9d7f-4e66-acd6-d1805fb86266",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create Bitsandbytes configuration\n",
    "compute_dtype = getattr(torch, \"float16\")\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type='nf4',\n",
    "        bnb_4bit_compute_dtype=compute_dtype,\n",
    "        bnb_4bit_use_double_quant=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b051cc13-90e1-4c3d-bc2d-63ca8fa5dd07",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForQuestionAnswering.from_pretrained(\"bert-base-uncased\",quantization_config=bnb_config)\n",
    "#model = BertForQuestionAnswering.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ab62f478-f877-4c5e-9edc-3f15928c330c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f6f0d6a1-087d-44e9-86e1-77a5c90ce1c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# enable gradient check pointing\n",
    "model.gradient_checkpointing_enable()\n",
    "# enable quantized training\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0eb3dd94-30eb-45f2-97f2-0b72db835593",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#model.modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dc81130a-a2f4-41dc-af97-3287814a0063",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# LoRA config\n",
    "config = LoraConfig(\n",
    "    r=2,\n",
    "    lora_alpha=8,\n",
    "    target_modules=['q_proj',\n",
    "        'k_proj',\n",
    "        'v_proj',\n",
    "        'dense'],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"QUESTION_ANS\"\n",
    ")\n",
    "\n",
    "# LoRA trainable version of model\n",
    "model = get_peft_model(model, config)\n",
    "\n",
    "# trainable parameter count\n",
    "#model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ab5bc92e-3e0b-457c-a5b0-23819df3b061",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "    \"\"\"\n",
    "    Read SQuAD data from a JSON file.\n",
    "\n",
    "    Parameters:\n",
    "    - path: Path to the JSON file containing SQuAD data\n",
    "\n",
    "    Returns:\n",
    "    - contexts: List of contexts (passages)\n",
    "    - questions: List of questions\n",
    "    - answers: List of answers\n",
    "    \"\"\"\n",
    "    # Open the JSON file and load the data\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        squad = json.load(f)\n",
    "\n",
    "    # Initialize lists to store contexts, questions, and answers\n",
    "    contexts = []\n",
    "    questions = []\n",
    "    answers = []\n",
    "\n",
    "    # Iterate over groups in the SQuAD data\n",
    "    for group in squad.get('data', []):\n",
    "        # Iterate over paragraphs in the group\n",
    "        for passage in group.get('paragraphs', []):\n",
    "            # Get the context (passage)\n",
    "            context = passage.get('context', '')\n",
    "            # Iterate over questions and answers in the paragraph\n",
    "            for qa in passage.get('qas', []):\n",
    "                # Get the question\n",
    "                question = qa.get('question', '')\n",
    "                # Iterate over answers for the question\n",
    "                for answer in qa.get('answers', []):\n",
    "                    # Append context, question, and answer to their respective lists\n",
    "                    contexts.append(context)\n",
    "                    questions.append(question)\n",
    "                    answers.append(answer)\n",
    "\n",
    "    # Return the lists of contexts, questions, and answers\n",
    "    return contexts, questions, answers\n",
    "\n",
    "def add_end_index(answers, contexts):\n",
    "    for answer, context in zip(answers, contexts):\n",
    "        gold_text = answer['text']\n",
    "        start_idx = answer['answer_start']\n",
    "        end_idx = start_idx + len(gold_text)\n",
    "\n",
    "        # Check if the answer is correctly positioned\n",
    "        for offset in [0, -1, -2]:\n",
    "            if context[start_idx + offset:end_idx + offset] == gold_text:\n",
    "                # Update answer start and end indices\n",
    "                answer['answer_start'] = start_idx + offset\n",
    "                answer['answer_end'] = end_idx + offset\n",
    "                break  # Break loop once correct offset is found\n",
    "\n",
    "def add_token_positions(encodings, answers):\n",
    "    \"\"\"\n",
    "    Adds token positions for answers to encodings.\n",
    "\n",
    "    Parameters:\n",
    "    - encodings: Encodings object containing tokenized inputs\n",
    "    - answers: List of dictionaries containing answer positions\n",
    "\n",
    "    Returns:\n",
    "    None (modifies encodings in place)\n",
    "    \"\"\"\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    # Loop through each answer\n",
    "    for i, answer in enumerate(answers):\n",
    "        # Convert character positions to token positions\n",
    "        start_positions.append(encodings.char_to_token(i, answer['answer_start']))\n",
    "        end_positions.append(encodings.char_to_token(i, answer['answer_end'] - 1))\n",
    "\n",
    "        # Handle cases where answer passage has been truncated\n",
    "        if start_positions[-1] is None:\n",
    "            start_positions[-1] = tokenizer.model_max_length\n",
    "        if end_positions[-1] is None:\n",
    "            end_positions[-1] = tokenizer.model_max_length\n",
    "\n",
    "    # Update encodings with start and end positions\n",
    "    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n",
    "\n",
    "class SQuAD_Dataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Custom dataset class for SQuAD.\n",
    "\n",
    "    Parameters:\n",
    "    - encodings: Encodings object containing tokenized inputs\n",
    "    \"\"\"\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves an item from the dataset.\n",
    "\n",
    "        Parameters:\n",
    "        - idx: Index of the item to retrieve\n",
    "\n",
    "        Returns:\n",
    "        Dictionary containing tensors for each key in the encodings\n",
    "        \"\"\"\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the length of the dataset.\n",
    "\n",
    "        Returns:\n",
    "        Integer representing the length of the dataset\n",
    "        \"\"\"\n",
    "        return len(self.encodings.input_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "02187df7-9667-499e-aa7b-714804e3269e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read training data\n",
    "contexts, questions, answers = read_data('/accounts/grad/fangyuan_li/259/data/train-v2.0.json')\n",
    "# Read validation data\n",
    "valid_contexts, valid_questions, valid_answers = read_data('/accounts/grad/fangyuan_li/259/data/val-v2.0.json')\n",
    "# Split train-v2.0 into train and test sets\n",
    "train_contexts = contexts[5000:]\n",
    "train_questions = questions[5000:]\n",
    "train_answers = answers[5000:]\n",
    "\n",
    "test_contexts = contexts[:5000]\n",
    "test_questions = questions[:5000]\n",
    "test_answers = answers[:5000]\n",
    "\n",
    "# Add indexes\n",
    "add_end_index(train_answers, train_contexts)\n",
    "add_end_index(valid_answers, valid_contexts)\n",
    "add_end_index(test_answers, test_contexts)\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "train_encodings = tokenizer(train_contexts, train_questions, truncation=True, padding=True)\n",
    "valid_encodings = tokenizer(valid_contexts, valid_questions, truncation=True, padding=True)\n",
    "test_encodings = tokenizer(test_contexts, test_questions, truncation=True, padding=True)\n",
    "\n",
    "# Add token positions for training data\n",
    "add_token_positions(train_encodings, train_answers)\n",
    "# Add token positions for validation data\n",
    "add_token_positions(valid_encodings, valid_answers)\n",
    "# Add token positions for test data\n",
    "add_token_positions(test_encodings, test_answers)\n",
    "\n",
    "# Create training dataset\n",
    "train_dataset = SQuAD_Dataset(train_encodings)\n",
    "# Create validation dataset\n",
    "valid_dataset = SQuAD_Dataset(valid_encodings)\n",
    "# Create test dataset\n",
    "test_dataset = SQuAD_Dataset(test_encodings)\n",
    "\n",
    "# Define the dataloaders\n",
    "# train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "# valid_loader = DataLoader(valid_dataset, batch_size=16)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2c8959ee-cc47-4645-94eb-7c7c18c8de4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9825b0e2-15f3-4031-8119-83e2361530a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lr = 2e-4\n",
    "batch_size = 16\n",
    "num_epochs = 5\n",
    "\n",
    "training_args = transformers.TrainingArguments(\n",
    "    output_dir = \"../test_trainer\",\n",
    "    num_train_epochs = num_epochs,\n",
    "    learning_rate = lr,\n",
    "    per_device_train_batch_size = batch_size,\n",
    "    per_device_eval_batch_size = batch_size,\n",
    "    weight_decay = 0.01,\n",
    "    logging_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    load_best_model_at_end = True,\n",
    "    gradient_accumulation_steps = 4,\n",
    "    warmup_steps = 2,\n",
    "    optim = \"paged_adamw_8bit\",\n",
    "    eval_strategy = \"epoch\"\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5789ed59-d62b-4410-bd6e-fa49fcd6a773",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = transformers.Trainer(\n",
    "    model = model,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = valid_dataset,\n",
    "    args = training_args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5317d732-97a2-4347-a024-bb1c1df96d7f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/accounts/grad/fangyuan_li/.local/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6390' max='6390' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6390/6390 3:46:46, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2.289700</td>\n",
       "      <td>1.462289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.264300</td>\n",
       "      <td>1.236898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.159600</td>\n",
       "      <td>1.198119</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/accounts/grad/fangyuan_li/.local/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/accounts/grad/fangyuan_li/.local/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/accounts/grad/fangyuan_li/.local/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/accounts/grad/fangyuan_li/.local/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6390, training_loss=1.464742597839642, metrics={'train_runtime': 13608.0216, 'train_samples_per_second': 30.064, 'train_steps_per_second': 0.47, 'total_flos': 1.0713658910662656e+17, 'train_loss': 1.464742597839642, 'epoch': 4.998044583496284})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.use_cache = False\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a884ad5f-5894-4402-a615-3db21289016d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"../full_data/QLoRA2/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
